---
title: "R Notebook"
output: html_notebook
---

```{r}
Sys.setenv("python" = "~/mianiconda3/envs/XAI/bin/python3.7")
library("reticulate")
reticulate::use_python(normalizePath("~/miniconda3/envs/XAI/bin/python3"), required = TRUE)
reticulate::use_condaenv("XAI", required = TRUE)
sys <- import("sys")
{ cat(sys$version); cat("\nPath: ", head(sys$path, 1)) }
```
```{r}
py_run_file(file = "~/mnist_loader.py")
mnist_idx <- lapply(py$mnist_idx, function(x) x$numpy())
```

```{r}
##### import modules that are needed ################################################
torch      = import("torch")
nn         = import("torch.nn") 
dsets      = import("torchvision.datasets") 
transforms = import("torchvision.transforms")
Variable   = import("torch.autograd")$Variable
```

```{r}
####### variables need to be carefully set to int, otherwise python won't accept it as float ########
input_size    = 784L
num_classes   = 10L
num_epochs    = 12L
batch_size    = 128L
learning_rate = 0.001


# transforms$Compose(transforms = list(transforms$ToTensor()))

train_dataset <- dsets$MNIST(root='data', train = TRUE, transform = transforms$ToTensor(), download = TRUE)
test_dataset <- dsets$MNIST(root='data', train = FALSE, transform = transforms$ToTensor())

## Data loaders
# py_run_string("loader = DataLoader(MNIST, batch_size=128, shuffle=True)")
# train_loader <- torch$utils$data$DataLoader(py$MNIST, batch_size = batch_size, shuffle = TRUE)
# torch$utils$data$DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = TRUE)
test_loader <- torch$utils$data$DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = FALSE)
```


```{r}
py_run_file(file = "~/mnist_net.py")

## Set devices to run on 
torch$manual_seed(1234)
use_cuda <- torch$cuda$is_available()
device <- torch$device(ifelse(use_cuda, "cuda", "cpu"))

## Model
model <- py$ConvNet()

## Loss + optimizer
criterion <- nn$CrossEntropyLoss()  
# optimizer <- torch$optim$SGD(model$parameters(), lr = learning_rate)  
optimizer <- torch$optim$Adadelta(model$parameters(), lr=1.0, rho=0.95, weight_decay=0.0)

## Training images
utils <- import_builtins()
train_enum <- utils$enumerate(py$loader)
train_it <- iterate(train_enum, simplify = FALSE) ## iterator over the batches

## Generate a function to get the training set activations 
## Assumes train_it and model are defined
activations <- function(){
  py_run_string("outputs= []
def hook(module, input, output):
    outputs.append(output)")
  function(){
    res <- vector(mode = "list", length = 4)
    ## Register hooks 
    l1_hook <- model$layer1$register_forward_hook(hook = py$hook)
    l2_hook <- model$layer2$register_forward_hook(hook = py$hook)
    fc1_hook <- model$fc1$register_forward_hook(hook = py$hook)
    fc2_hook <- model$fc2$register_forward_hook(hook = py$hook)
    for (batch in train_it){
      
      ## Do forward pass 
      images <- Variable(batch[[2]][[1]])
      outputs <- model$forward(images)
      predicted <- torch$max(outputs$data, 1L)
      
      ## Get hook outputs 
      res[[1]] <- py$outputs[[1]]
      res[[2]] <- py$outputs[[2]]
      res[[3]] <- py$outputs[[3]]
      res[[4]] <- py$outputs[[4]]
    }
    l1_hook$remove()
    l2_hook$remove()
    fc1_hook$remove()
    fc2_hook$remove()
    py$outputs <- list()
    return(res)
  }
}
get_activations <- activations()

# Generate function to get the accuracy on the testing data
# Assumes test_it is defined
test_enum <- utils$enumerate(test_loader)
test_it <- iterate(test_enum, simplify = FALSE)
test_accuracy <- function(){
  total <- length(test_loader$dataset$test_labels$numpy())
  function(){
    correct <- 0
    for (batch in test_it){
      images <- Variable(batch[[2]][[1]])
      labels <- Variable(batch[[2]][[2]])
      outputs <- model$forward(images)
      predicted <- torch$max(outputs$data, 1L)
      correct <- correct + sum(as.vector(predicted[[2]]$numpy()) == labels$data$numpy())
    }
    return(correct/total)
  }
}
get_test_accuracy <- test_accuracy()

## Thresholds and results
acc_thresholds <- c(seq(50, 90, by = 5), seq(91, 99))/100
act_res <- vector(mode="list", length = length(acc_thresholds))

cc <- 1L
for (epoch in 1:num_epochs)
{
  { i <- 0; correct <- 0; model$train() }
  for (img in train_it) {
    i <- i+1
    batch_idx <- img[[1]]
    images <- Variable(img[[2]][[1]]) # $view(-1L, 28L*28L))
    labels <- Variable(img[[2]][[2]])

    ## Forward + Backward + Optimize
    optimizer$zero_grad() ## PyTorch accumulates gradients by default, so they need to be zeroed each batch
    outputs <- model$forward(images)
    loss <- criterion(outputs, labels)
    loss$backward()
    optimizer$step()

    ## Get the predicted 
    predicted <- torch$max(outputs$data, 1L)
    correct <- correct + sum(as.vector(predicted[[2]]$numpy()) == labels$data$numpy())
    
    ## Register hooks to extract activations 
    if (cc <= length(acc_thresholds)){
      c_acc <- get_test_accuracy()
      if (c_acc > acc_thresholds[cc]){
        saveRDS(get_activations(), file = sprintf("acc_%s.rds", acc_thresholds[cc]))
        cc <- cc+1L
      }
    }

    ## Status 
    if (i %% 100 == 0){
      cat("epoch ");cat(epoch); cat(" / ");cat(num_epochs);
      cat(" step "); cat(i+1)
      cat(" loss: ");cat(loss$data$numpy())
      cat("\n")
    }
  }
}

## The first batch to have a model accuracy above each of these thresholds
## registers a hook to save the activation outputs
acc_thresholds <- 






sprintf("Accuracy of the model on the 10000 test images: %f  ", correct/10000)

# kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
```



```{r}
## Function to extract the activation 'point clouds' of a CNN model
##    x := the mnist data set 
##    idx := rows to extract the activations of 
##    layers := which layers to get the activations output from 
getActivations <- function(model, x, idx, layers){
  ## Get the layer outputs
  input <- model$input
  layer_outputs <- lapply(model$layers, function(layer) layer$output)

  ## Make the model to get the activation outputs
  activations_model <- keras_model(inputs = model$input, outputs = layer_outputs)

  ## Allocate the outputs
  res <- predict(object = activations_model, x = array_reshape(x[idx, 1:28, 1:28], c(length(idx), 28, 28, 1)), batch_size = 1)
  act_point_clouds <- lapply(layers, function(i){
    d_len <- prod(dim(res[[i]])[-1])
    n_len <- dim(res[[i]])[[1]]
    keras::k_reshape(res[[i]], shape = c(n_len, d_len))$eval()
  })
  
  ## Return named list of layer outputs
  names(act_point_clouds) <- sapply(layers, function(i){ keras::get_config(keras::get_layer(model, index = i))$name })
  return(act_point_clouds)
}

## Choose the subsets to work with 
set.seed(1234)
class_labels <- sort(unique(mnist$train$y))
sample_idx <- lapply(class_labels, function(y_i) { sample(x = which(mnist$train$y == y_i), size = 500, replace = FALSE) })

## Get the activations as the net trains
mnist_activations <- lapply(1:12, function(e_i){
  model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = 1,
  verbose = 1,
  validation_data = list(x_test, y_test)
  )
  ## Get the output activations of the subset for the given layers 
  sess <- keras::use_session_with_seed(1234, disable_gpu = FALSE, disable_parallel_cpu = FALSE)
  with(sess$as_default(), {
    getActivations(model = model, x = mnist$train$x, idx = unlist(sample_idx), layers = c(1, 2, 6, 8))
  })
})


scores <- model %>% evaluate(
  x_test, y_test, verbose = 0
)

```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

